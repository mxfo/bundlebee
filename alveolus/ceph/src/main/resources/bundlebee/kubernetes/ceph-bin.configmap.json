{
  "apiVersion": "v1",
  "kind": "ConfigMap",
  "metadata": {
    "name": "ceph-bin"
  },
  "data": {
    "ceph-key.py": "#!/bin/python\nimport os\nimport struct\nimport time\nimport base64\nkey = os.urandom(16)\nheader = struct.pack(\n    '<hiih',\n    1,                 # le16 type: CEPH_CRYPTO_AES\n    int(time.time()),  # le32 created: seconds\n    0,                 # le32 created: nanoseconds,\n    len(key),          # le16: len(key)\n)\nprint(base64.b64encode(header + key).decode('ascii'))\n\n",
    "ceph-key.sh": "#!/bin/bash\n\n\n\nset -ex\n\nfunction ceph_gen_key () {\n  python ${CEPH_GEN_DIR}/ceph-key.py\n}\n\nfunction kube_ceph_keyring_gen () {\n  CEPH_KEY=$1\n  CEPH_KEY_TEMPLATE=$2\n  sed \"s|{{ key }}|${CEPH_KEY}|\" ${CEPH_TEMPLATES_DIR}/${CEPH_KEY_TEMPLATE} | base64 | tr -d '\\n'\n}\n\nfunction create_kube_key () {\n  CEPH_KEYRING=$1\n  CEPH_KEYRING_NAME=$2\n  CEPH_KEYRING_TEMPLATE=$3\n  KUBE_SECRET_NAME=$4\n  if ! kubectl get --namespace ${DEPLOYMENT_NAMESPACE} secrets ${KUBE_SECRET_NAME}; then\n    {\n      cat <<EOF\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${KUBE_SECRET_NAME}\ntype: Opaque\ndata:\n  ${CEPH_KEYRING_NAME}: $( kube_ceph_keyring_gen ${CEPH_KEYRING} ${CEPH_KEYRING_TEMPLATE} )\nEOF\n    } | kubectl create --namespace ${DEPLOYMENT_NAMESPACE} -f -\n  fi\n}\n\n#create_kube_key <ceph_key> <ceph_keyring_name> <ceph_keyring_template> <kube_secret_name>\ncreate_kube_key $(ceph_gen_key) ${CEPH_KEYRING_NAME} ${CEPH_KEYRING_TEMPLATE} ${KUBE_SECRET_NAME}\n\n",
    "ceph-storage-key.sh": "#!/bin/bash\n\n\n\nset -ex\n\nfunction ceph_gen_key () {\n  python ${CEPH_GEN_DIR}/ceph-key.py\n}\n\nfunction kube_ceph_keyring_gen () {\n  CEPH_KEY=$1\n  CEPH_KEY_TEMPLATE=$2\n  sed \"s|{{ key }}|${CEPH_KEY}|\" ${CEPH_TEMPLATES_DIR}/${CEPH_KEY_TEMPLATE} | base64 | tr -d '\\n'\n}\n\nCEPH_CLIENT_KEY=$(ceph_gen_key)\n\nfunction create_kube_key () {\n  CEPH_KEYRING=$1\n  CEPH_KEYRING_NAME=$2\n  CEPH_KEYRING_TEMPLATE=$3\n  KUBE_SECRET_NAME=$4\n\n  if ! kubectl get --namespace ${DEPLOYMENT_NAMESPACE} secrets ${KUBE_SECRET_NAME}; then\n    {\n      cat <<EOF\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${KUBE_SECRET_NAME}\ntype: Opaque\ndata:\n  ${CEPH_KEYRING_NAME}: $( kube_ceph_keyring_gen ${CEPH_KEYRING} ${CEPH_KEYRING_TEMPLATE} )\nEOF\n    } | kubectl create --namespace ${DEPLOYMENT_NAMESPACE} -f -\n  fi\n}\n#create_kube_key <ceph_key> <ceph_keyring_name> <ceph_keyring_template> <kube_secret_name>\ncreate_kube_key ${CEPH_CLIENT_KEY} ${CEPH_KEYRING_NAME} ${CEPH_KEYRING_TEMPLATE} ${CEPH_KEYRING_ADMIN_NAME}\n\nfunction create_kube_storage_key () {\n  CEPH_KEYRING=$1\n  KUBE_SECRET_NAME=$2\n\n  if ! kubectl get --namespace ${DEPLOYMENT_NAMESPACE} secrets ${KUBE_SECRET_NAME}; then\n    {\n      cat <<EOF\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${KUBE_SECRET_NAME}\ntype: kubernetes.io/rbd\ndata:\n  key: $( echo ${CEPH_KEYRING} | base64 | tr -d '\\n' )\nEOF\n    } | kubectl create --namespace ${DEPLOYMENT_NAMESPACE} -f -\n  fi\n}\n#create_kube_storage_key <ceph_key> <kube_secret_name>\ncreate_kube_storage_key ${CEPH_CLIENT_KEY} ${CEPH_STORAGECLASS_ADMIN_SECRET_NAME}\n\n",
    "init_dirs.sh": "#!/bin/bash\n\n\n\nset -ex\nexport LC_ALL=C\nsource variables_entrypoint.sh\n\nfor keyring in $OSD_BOOTSTRAP_KEYRING $MDS_BOOTSTRAP_KEYRING $RGW_BOOTSTRAP_KEYRING; do\n  mkdir -p $(dirname $keyring)\ndone\n\n# Let's create the ceph directories\nfor directory in mon osd mds radosgw tmp mgr; do\n  mkdir -p /var/lib/ceph/$directory\ndone\n\n# Make the monitor directory\nmkdir -p \"$MON_DATA_DIR\"\n\n# Create socket directory\nmkdir -p /run/ceph\n\n# Creating rados directories\nmkdir -p /var/lib/ceph/radosgw/${RGW_NAME}\n\n# Create the MDS directory\nmkdir -p /var/lib/ceph/mds/${CLUSTER}-${MDS_NAME}\n\n# Create the MGR directory\nmkdir -p /var/lib/ceph/mgr/${CLUSTER}-$MGR_NAME\n\n# Adjust the owner of all those directories\nchown -R ceph. /run/ceph/ /var/lib/ceph/*\n\n",
    "common_functions.sh": "#!/bin/bash\nset -ex\n\n# log arguments with timestamp\nfunction log {\n  if [ -z \"$*\" ]; then\n    return 1\n  fi\n\n  TIMESTAMP=$(date '+%F %T')\n  echo \"${TIMESTAMP}  $0: $*\"\n  return 0\n}\n\n# Given two strings, return the length of the shared prefix\nfunction prefix_length {\n  local maxlen=${#1}\n  for ((i=maxlen-1;i>=0;i--)); do\n    if [[ \"${1:0:i}\" == \"${2:0:i}\" ]]; then\n      echo $i\n      return\n    fi\n  done\n}\n\n# Test if a command line tool is available\nfunction is_available {\n  command -v $@ &>/dev/null\n}\n\n# Calculate proper device names, given a device and partition number\nfunction dev_part {\n  local osd_device=${1}\n  local osd_partition=${2}\n\n  if [[ -L ${osd_device} ]]; then\n    # This device is a symlink. Work out it's actual device\n    local actual_device=$(readlink -f ${osd_device})\n    local bn=$(basename ${osd_device})\n    if [[ \"${actual_device:0-1:1}\" == [0-9] ]]; then\n      local desired_partition=\"${actual_device}p${osd_partition}\"\n    else\n      local desired_partition=\"${actual_device}${osd_partition}\"\n    fi\n    # Now search for a symlink in the directory of $osd_device\n    # that has the correct desired partition, and the longest\n    # shared prefix with the original symlink\n    local symdir=$(dirname ${osd_device})\n    local link=\"\"\n    local pfxlen=0\n    for option in $(ls $symdir); do\n    if [[ $(readlink -f $symdir/$option) == $desired_partition ]]; then\n      local optprefixlen=$(prefix_length $option $bn)\n      if [[ $optprefixlen > $pfxlen ]]; then\n        link=$symdir/$option\n        pfxlen=$optprefixlen\n      fi\n    fi\n    done\n    if [[ $pfxlen -eq 0 ]]; then\n      >&2 log \"Could not locate appropriate symlink for partition ${osd_partition} of ${osd_device}\"\n      exit 1\n    fi\n    echo \"$link\"\n  elif [[ \"${osd_device:0-1:1}\" == [0-9] ]]; then\n    echo \"${osd_device}p${osd_partition}\"\n  else\n    echo \"${osd_device}${osd_partition}\"\n  fi\n}\n\n# Wait for a file to exist, regardless of the type\nfunction wait_for_file {\n  timeout 10 bash -c \"while [ ! -e ${1} ]; do echo 'Waiting for ${1} to show up' && sleep 1 ; done\"\n}\n\nfunction get_osd_path {\n  echo \"$OSD_PATH_BASE-$1/\"\n}\n\n# Bash substitution to remove everything before '='\n# and only keep what is after\nfunction extract_param {\n  echo \"${1##*=}\"\n}\n\n",
    "osd_activate_journal.sh": "#!/bin/bash\n# NOTE: This does not appear to function correctly in containers,\n# as ceph-disk activate-journal attempts to start the OSD using\n# the init subsystem, which does not exist here.  It should probably\n# go away entirely.\nset -ex\n\nfunction osd_activate_journal {\n  if [[ -z \"${OSD_JOURNAL}\" ]];then\n    log \"ERROR- You must provide a device to build your OSD journal ie: /dev/sdb2\"\n    exit 1\n  fi\n\n  if [ ! -e \"${OSD_JOURNAL}\" ]; then\n    log \"ERROR- Journal ${OSD_JOURNAL} does not exist\"\n    exit 1\n   fi\n\n  if [ ! -b \"${OSD_JOURNAL}\" ]; then\n   # no need to activate non-device journals\n   return 0;\n  fi\n\n  # watch the udev event queue, and exit if all current events are handled\n  udevadm settle --timeout=600\n\n  # wait till partition exists\n  wait_for_file ${OSD_JOURNAL}\n\n  chown ceph. ${OSD_JOURNAL}\n  ceph-disk -v --setuser ceph --setgroup disk activate-journal ${OSD_JOURNAL}\n}\n\n",
    "osd_common.sh": "# Start the latest OSD\n# In case of forego, we don't run ceph-osd, start_forego will do it later\nfunction start_osd() {\n  mode=$1 #forego or empty\n\n  OSD_ID=$(cat /var/lib/ceph/osd/$(ls -ltr /var/lib/ceph/osd/ | tail -n1 | awk -v pattern=\"$CLUSTER\" '$0 ~ pattern {print $9}')/whoami)\n  OSD_PATH=$(get_osd_path $OSD_ID)\n  OSD_KEYRING=\"$OSD_PATH/keyring\"\n  OSD_WEIGHT=$(df -P -k $OSD_PATH | tail -1 | awk '{ d= $2/1073741824 ; r = sprintf(\"%.2f\", d); print r }')\n  ceph ${CLI_OPTS} --name=osd.${OSD_ID} --keyring=$OSD_KEYRING osd crush create-or-move -- ${OSD_ID} ${OSD_WEIGHT} ${CRUSH_LOCATION}\n\n  # ceph-disk activiate has exec'ed /usr/bin/ceph-osd ${CLI_OPTS} -f -i ${OSD_ID}\n  # wait till docker stop or ceph-osd is killed\n  OSD_PID=$(ps -ef |grep ceph-osd |grep osd.${OSD_ID} |awk '{print $2}')\n  if [ -n \"${OSD_PID}\" ]; then\n      log \"OSD (PID ${OSD_PID}) is running, waiting till it exits\"\n      while [ -e /proc/${OSD_PID} ]; do sleep 1;done\n  fi\n\n  if [[ \"$mode\" == \"forego\" ]]; then\n   echo \"${CLUSTER}-${OSD_ID}: /usr/bin/ceph-osd ${CLI_OPTS} -f -i ${OSD_ID} --setuser ceph --setgroup disk\" | tee -a /etc/forego/${CLUSTER}/Procfile\n  else\n   log \"SUCCESS\"\n   exec /usr/bin/ceph-osd ${CLI_OPTS} -f -i ${OSD_ID} --setuser ceph --setgroup disk\n  fi\n}\n\n# Starting forego\nfunction start_forego() {\n   exec /usr/local/bin/forego start -f /etc/forego/${CLUSTER}/Procfile\n}\n\n",
    "osd_directory.sh": "#!/bin/bash\nset -ex\n\nfunction is_integer {\n  # This function is about saying if the passed argument is an integer\n  # Supports also negative integers\n  # We use $@ here to consider everything given as parameter and not only the\n  # first one : that's mainly for splited strings like \"10 10\"\n  [[ $@ =~ ^-?[0-9]+$ ]]\n}\n\nfunction osd_directory {\n  local test_luminous=$(ceph -v | egrep -q \"12.2|luminous\"; echo $?)\n  if [[ ${test_luminous} -ne 0 ]]; then\n      log \"ERROR- need Luminous release\"\n      exit 1\n  fi\n\n  if [[ ! -d /var/lib/ceph/osd ]]; then\n    log \"ERROR- could not find the osd directory, did you bind mount the OSD data directory?\"\n    log \"ERROR- use -v <host_osd_data_dir>:/var/lib/ceph/osd\"\n    exit 1\n  fi\n\n  if [ -z \"${HOSTNAME}\" ]; then\n    log \"HOSTNAME not set; This will prevent to add an OSD into the CRUSH map\"\n    exit 1\n  fi\n\n  chown ceph. /var/log/ceph\n\n  # check if anything is present, if not, create an osd and its directory\n  if [[ -n \"$(find /var/lib/ceph/osd -prune -empty)\" ]]; then\n    log \"Creating osd\"\n    UUID=$(uuidgen)\n    OSD_SECRET=$(ceph-authtool --gen-print-key)\n    OSD_ID=$(echo \"{\\\"cephx_secret\\\": \\\"${OSD_SECRET}\\\"}\" | ceph osd new ${UUID} -i - -n client.bootstrap-osd -k \"$OSD_BOOTSTRAP_KEYRING\")\n    if is_integer \"$OSD_ID\"; then\n      log \"OSD created with ID: ${OSD_ID}\"\n    else\n      log \"OSD creation failed: ${OSD_ID}\"\n      exit 1\n    fi\n\n    OSD_PATH=$(get_osd_path \"$OSD_ID\")\n    if [ -n \"${JOURNAL_DIR}\" ]; then\n       OSD_J=\"${JOURNAL_DIR}/journal.${OSD_ID}\"\n       chown -R ceph. ${JOURNAL_DIR}\n    else\n       if [ -n \"${JOURNAL}\" ]; then\n          OSD_J=${JOURNAL}\n          chown -R ceph. $(dirname ${JOURNAL_DIR})\n       else\n          OSD_J=${OSD_PATH}/journal\n       fi\n    fi\n    # create the folder and own it\n    mkdir -p \"$OSD_PATH\"\n    chown \"${CHOWN_OPT[@]}\" ceph. \"$OSD_PATH\"\n    log \"created folder $OSD_PATH\"\n    # write the secret to the osd keyring file\n    ceph-authtool --create-keyring ${OSD_PATH}/keyring --name osd.${OSD_ID} --add-key ${OSD_SECRET}\n    OSD_KEYRING=\"$OSD_PATH/keyring\"\n    # init data directory\n    ceph-osd -i ${OSD_ID} --mkfs --osd-uuid ${UUID} --mkjournal --osd-journal ${OSD_J} --setuser ceph --setgroup ceph\n    # add the osd to the crush map\n    OSD_WEIGHT=$(df -P -k $OSD_PATH | tail -1 | awk '{ d= $2/1073741824 ; r = sprintf(\"%.2f\", d); print r }')\n    ceph --name=osd.${OSD_ID} --keyring=${OSD_KEYRING} osd crush create-or-move -- ${OSD_ID} ${OSD_WEIGHT} ${CRUSH_LOCATION}\n  fi\n\n  # create the directory and an empty Procfile\n  mkdir -p /etc/forego/\"${CLUSTER}\"\n  echo \"\" > /etc/forego/\"${CLUSTER}\"/Procfile\n\n  for OSD_ID in $(ls /var/lib/ceph/osd | sed 's/.*-//'); do\n    OSD_PATH=$(get_osd_path \"$OSD_ID\")\n    OSD_KEYRING=\"$OSD_PATH/keyring\"\n    if [ -n \"${JOURNAL_DIR}\" ]; then\n       OSD_J=\"${JOURNAL_DIR}/journal.${OSD_ID}\"\n       chown -R ceph. ${JOURNAL_DIR}\n    else\n       if [ -n \"${JOURNAL}\" ]; then\n          OSD_J=${JOURNAL}\n          chown -R ceph. $(dirname ${JOURNAL_DIR})\n       else\n          OSD_J=${OSD_PATH}/journal\n       fi\n    fi\n    # log osd filesystem type\n    FS_TYPE=`stat --file-system -c \"%T\" ${OSD_PATH}`\n    log \"OSD $OSD_PATH filesystem type: $FS_TYPE\"\n    echo \"${CLUSTER}-${OSD_ID}: /usr/bin/ceph-osd ${CLI_OPTS[*]} -f -i ${OSD_ID} --osd-journal ${OSD_J} -k $OSD_KEYRING\" | tee -a /etc/forego/\"${CLUSTER}\"/Procfile\n  done\n  log \"SUCCESS\"\n  start_forego\n}\n\n",
    "osd_directory_single.sh": "#!/bin/bash\nset -ex\n\nfunction osd_directory_single {\n  if [[ ! -d /var/lib/ceph/osd ]]; then\n    log \"ERROR- could not find the osd directory, did you bind mount the OSD data directory?\"\n    log \"ERROR- use -v <host_osd_data_dir>:/var/lib/ceph/osd\"\n    exit 1\n  fi\n\n  # pick one osd and make sure no lock is held\n  for OSD_ID in $(ls /var/lib/ceph/osd | sed 's/.*-//'); do\n    OSD_PATH=$(get_osd_path $OSD_ID)\n    OSD_KEYRING=\"$OSD_PATH/keyring\"\n\n    if [[ -n \"$(find $OSD_PATH -prune -empty)\" ]]; then\n      log \"Looks like OSD: ${OSD_ID} has not been bootstrapped yet, doing nothing, moving on to the next discoverable OSD\"\n    else\n      # check if the osd has a lock, if yes moving on, if not we run it\n      # many thanks to Julien Danjou for the python piece\n      if python -c \"import sys, fcntl, struct; l = fcntl.fcntl(open('${OSD_PATH}/fsid', 'a'), fcntl.F_GETLK, struct.pack('hhllhh', fcntl.F_WRLCK, 0, 0, 0, 0, 0)); l_type, l_whence, l_start, l_len, l_pid, l_sysid = struct.unpack('hhllhh', l); sys.exit(0 if l_type == fcntl.F_UNLCK else 1)\"; then\n        log \"Looks like OSD: ${OSD_ID} is not started, starting it...\"\n        log \"SUCCESS\"\n        exec ceph-osd $DAEMON_OPTS -i ${OSD_ID} -k $OSD_KEYRING\n        break\n      fi\n    fi\n  done\n  log \"Looks like all the OSDs are already running, doing nothing\"\n  log \"Exiting the container\"\n  log \"SUCCESS\"\n  exit 0\n}\n\n",
    "osd_disk_activate.sh": "#!/bin/bash\nset -ex\n\nfunction osd_activate {\n  if [[ -z \"${OSD_DEVICE}\" ]];then\n    log \"ERROR- You must provide a device to build your OSD ie: /dev/sdb\"\n    exit 1\n  fi\n\n  CEPH_DISK_OPTIONS=\"\"\n  CEPH_OSD_OPTIONS=\"\"\n\n  DATA_UUID=$(blkid -o value -s PARTUUID ${OSD_DEVICE}*1)\n  LOCKBOX_UUID=$(blkid -o value -s PARTUUID ${OSD_DEVICE}3 || true)\n  JOURNAL_PART=$(dev_part ${OSD_DEVICE} 2)\n  ACTUAL_OSD_DEVICE=$(readlink -f ${OSD_DEVICE}) # resolve /dev/disk/by-* names\n\n  # watch the udev event queue, and exit if all current events are handled\n  udevadm settle --timeout=600\n\n  # wait till partition exists then activate it\n  if [[ -n \"${OSD_JOURNAL}\" ]]; then\n    wait_for_file ${OSD_JOURNAL}\n    chown ceph. ${OSD_JOURNAL}\n    CEPH_OSD_OPTIONS=\"${CEPH_OSD_OPTIONS} --osd-journal ${OSD_JOURNAL}\"\n  else\n    wait_for_file $(dev_part ${OSD_DEVICE} 1)\n    chown ceph. $JOURNAL_PART\n  fi\n\n  chown ceph. /var/log/ceph\n\n  DATA_PART=$(dev_part ${OSD_DEVICE} 1)\n  MOUNTED_PART=${DATA_PART}\n\n  if [[ ${OSD_DMCRYPT} -eq 1 ]]; then\n    echo \"Mounting LOCKBOX directory\"\n    # NOTE(leseb): adding || true so when this bug will be fixed the entrypoint will not fail\n    # Ceph bug tracker: http://tracker.ceph.com/issues/18945\n    mkdir -p /var/lib/ceph/osd-lockbox/${DATA_UUID}\n    mount /dev/disk/by-partuuid/${LOCKBOX_UUID} /var/lib/ceph/osd-lockbox/${DATA_UUID} || true\n    CEPH_DISK_OPTIONS=\"$CEPH_DISK_OPTIONS --dmcrypt\"\n    MOUNTED_PART=\"/dev/mapper/${DATA_UUID}\"\n  fi\n\n  ceph-disk -v --setuser ceph --setgroup disk activate ${CEPH_DISK_OPTIONS} --no-start-daemon ${DATA_PART}\n\n  OSD_ID=$(grep \"${MOUNTED_PART}\" /proc/mounts | awk '{print $2}' | grep -oh '[0-9]*')\n  OSD_PATH=$(get_osd_path $OSD_ID)\n  OSD_KEYRING=\"$OSD_PATH/keyring\"\n  OSD_WEIGHT=$(df -P -k $OSD_PATH | tail -1 | awk '{ d= $2/1073741824 ; r = sprintf(\"%.2f\", d); print r }')\n  ceph ${CLI_OPTS} --name=osd.${OSD_ID} --keyring=$OSD_KEYRING osd crush create-or-move -- ${OSD_ID} ${OSD_WEIGHT} ${CRUSH_LOCATION}\n\n  log \"SUCCESS\"\n  exec /usr/bin/ceph-osd ${CLI_OPTS} ${CEPH_OSD_OPTIONS} -f -i ${OSD_ID} --setuser ceph --setgroup disk\n}\n\n",
    "osd_disk_prepare.sh": "#!/bin/bash\nset -ex\n\nfunction osd_disk_prepare {\n  if [[ -z \"${OSD_DEVICE}\" ]];then\n    log \"ERROR- You must provide a device to build your OSD ie: /dev/sdb\"\n    exit 1\n  fi\n\n  if [[ ! -e \"${OSD_DEVICE}\" ]]; then\n    log \"ERROR- The device pointed by OSD_DEVICE ($OSD_DEVICE) doesn't exist !\"\n    exit 1\n  fi\n\n  if [ ! -e $OSD_BOOTSTRAP_KEYRING ]; then\n    log \"ERROR- $OSD_BOOTSTRAP_KEYRING must exist. You can extract it from your current monitor by running 'ceph auth get client.bootstrap-osd -o $OSD_BOOTSTRAP_KEYRING'\"\n    exit 1\n  fi\n  timeout 10 ceph ${CLI_OPTS} --name client.bootstrap-osd --keyring $OSD_BOOTSTRAP_KEYRING health || exit 1\n\n  # search for some ceph metadata on the disk\n  if [[ \"$(parted --script ${OSD_DEVICE} print | egrep '^ 1.*ceph data')\" ]]; then\n    # parted will trigger udev, so make sure all current udev event be handled.\n    udevadm settle --timeout=600 && partprobe ${OSD_DEVICE} && \\\n      udevadm settle --timeout=600\n    log \"It looks like ${OSD_DEVICE} is already an OSD\"\n    log \"Checking if it belongs to this cluster\"\n    tmp_osd_mount=\"/var/lib/ceph/tmp/`echo $RANDOM`/\"\n    mkdir -p $tmp_osd_mount\n    mount ${OSD_DEVICE}1 ${tmp_osd_mount}\n    osd_cluster_fsid=`cat ${tmp_osd_mount}/ceph_fsid`\n    umount ${tmp_osd_mount} && rmdir ${tmp_osd_mount}\n    cluster_fsid=`ceph ${CLI_OPTS} --name client.bootstrap-osd --keyring $OSD_BOOTSTRAP_KEYRING fsid`\n    if [ \"${osd_cluster_fsid}\" == \"${cluster_fsid}\" ]; then\n      log \"The OSD on ${OSD_DEVICE} belongs to this cluster, moving to activate phase\"\n      return\n    fi\n    log \"This OSD belonged to another cluster: ${osd_cluster_fsid}. Current cluster fsid: ${cluster_fsid}\"\n  fi\n\n  if [[ ${OSD_FORCE_ZAP} -eq 1 ]]; then\n    log \"Zapping ${OSD_DEVICE}\"\n    ceph-disk -v zap ${OSD_DEVICE}\n  fi\n\n  if [[ ${OSD_BLUESTORE} -eq 0 ]]; then\n    ceph-disk -v prepare ${CLI_OPTS} --filestore ${OSD_DEVICE}\n  elif [[ ${OSD_DMCRYPT} -eq 1 ]]; then\n    # the admin key must be present on the node\n    if [[ ! -e $ADMIN_KEYRING ]]; then\n      log \"ERROR- $ADMIN_KEYRING must exist; get it from your existing mon\"\n      exit 1\n    fi\n    # in order to store the encrypted key in the monitor's k/v store\n    ceph-disk -v prepare ${CLI_OPTS} --journal-uuid ${OSD_JOURNAL_UUID} --lockbox-uuid ${OSD_LOCKBOX_UUID} --dmcrypt ${OSD_DEVICE} ${OSD_JOURNAL}\n    echo \"Unmounting LOCKBOX directory\"\n    # NOTE(leseb): adding || true so when this bug will be fixed the entrypoint will not fail\n    # Ceph bug tracker: http://tracker.ceph.com/issues/18944\n    DATA_UUID=$(blkid -o value -s PARTUUID ${OSD_DEVICE}1)\n    umount /var/lib/ceph/osd-lockbox/${DATA_UUID} || true\n  else\n    ceph-disk -v prepare ${CLI_OPTS} --journal-uuid ${OSD_JOURNAL_UUID} ${OSD_DEVICE} ${OSD_JOURNAL}\n  fi\n\n  # watch the udev event queue, and exit if all current events are handled\n  udevadm settle --timeout=600\n\n  if [[ -n \"${OSD_JOURNAL}\" ]]; then\n    wait_for_file ${OSD_JOURNAL}\n    chown ceph. ${OSD_JOURNAL}\n  else\n    wait_for_file $(dev_part ${OSD_DEVICE} 2)\n    chown ceph. $(dev_part ${OSD_DEVICE} 2)\n  fi\n}\n\n",
    "osd_disks.sh": "#!/bin/bash\nset -ex\n\nfunction get_osd_dev {\n  for i in ${OSD_DISKS}\n   do\n    osd_id=$(echo ${i}|sed 's/\\(.*\\):\\(.*\\)/\\1/')\n    osd_dev=\"/dev/$(echo ${i}|sed 's/\\(.*\\):\\(.*\\)/\\2/')\"\n    if [ ${osd_id} = ${1} ]; then\n      echo -n \"${osd_dev}\"\n    fi\n  done\n}\n\nfunction osd_disks {\n  if [[ ! -d /var/lib/ceph/osd ]]; then\n    log \"ERROR- could not find the osd directory, did you bind mount the OSD data directory?\"\n    log \"ERROR- use -v <host_osd_data_dir>:/var/lib/ceph/osd\"\n    exit 1\n  fi\n  if [[  -z ${OSD_DISKS} ]]; then\n    log \"ERROR- could not find the osd devices, did you configure OSD disks?\"\n    log \"ERROR- use -e OSD_DISKS=\\\"0:sdd 1:sde 2:sdf\\\"\"\n    exit 1\n  fi\n\n  # Create the directory and an empty Procfile\n  mkdir -p /etc/forego/${CLUSTER}\n  echo \"\" > /etc/forego/${CLUSTER}/Procfile\n\n  # check if anything is there, if not create an osd with directory\n  if [[ -z \"$(find /var/lib/ceph/osd -prune -empty)\" ]]; then\n    log \"Mount existing and prepared OSD disks for ceph-cluster ${CLUSTER}\"\n    for OSD_ID in $(ls /var/lib/ceph/osd | sed 's/.*-//'); do\n      OSD_PATH=$(get_osd_path $OSD_ID)\n      OSD_KEYRING=\"$OSD_PATH/keyring\"\n      OSD_DEV=$(get_osd_dev ${OSD_ID})\n      if [[ -z ${OSD_DEV} ]]; then\n        log \"No device mapping for ${CLUSTER}-${OSD_ID} for ceph-cluster ${CLUSTER}\"\n        exit 1\n      fi\n      mount ${MOUNT_OPTS} $(dev_part ${OSD_DEV} 1) $OSD_PATH\n      xOSD_ID=$(cat $OSD_PATH/whoami)\n      if [[ \"${OSD_ID}\" != \"${xOSD_ID}\" ]]; then\n        log \"Device ${OSD_DEV} is corrupt for $OSD_PATH\"\n        exit 1\n      fi\n      echo \"${CLUSTER}-${OSD_ID}: /usr/bin/ceph-osd ${CLI_OPTS} -f -i ${OSD_ID} --setuser ceph --setgroup disk\" | tee -a /etc/forego/${CLUSTER}/Procfile\n    done\n    exec /usr/local/bin/forego start -f /etc/forego/${CLUSTER}/Procfile\n  fi\n\n  #\n  # As per the exec in the first statement, we only reach here if there is some OSDs\n  #\n  for OSD_DISK in ${OSD_DISKS}; do\n    OSD_DEV=\"/dev/$(echo ${OSD_DISK}|sed 's/\\(.*\\):\\(.*\\)/\\2/')\"\n\n    if [[ \"$(parted --script ${OSD_DEV} print | egrep '^ 1.*ceph data')\" ]]; then\n      # parted will trigger udev, so make sure all current udev event be handled.\n      udevadm settle --timeout=600 && partprobe ${OSD_DEV} && \\\n        udevadm settle --timeout=600\n      if [[ ${OSD_FORCE_ZAP} -eq 1 ]]; then\n        ceph-disk -v zap ${OSD_DEV}\n      else\n        log \"ERROR- It looks like the device ($OSD_DEV) is an OSD, set OSD_FORCE_ZAP=1 to use this device anyway and zap its content\"\n        exit 1\n      fi\n    fi\n\n    ceph-disk -v prepare ${CLI_OPTS} ${OSD_DEV} ${OSD_JOURNAL}\n\n    # prepare the OSDs configuration and start them later\n    start_osd forego\n  done\n\n  log \"SUCCESS\"\n  # Actually, starting them as per forego configuration\n  start_forego\n}\n\n",
    "start_mon.sh": "#!/bin/bash\nset -ex\nexport LC_ALL=C\n\nsource variables_entrypoint.sh\nsource common_functions.sh\n\nif [[ -z \"$CEPH_PUBLIC_NETWORK\" ]]; then\n  log \"ERROR- CEPH_PUBLIC_NETWORK must be defined as the name of the network for the OSDs\"\n  exit 1\nfi\n\nif [[ -z \"$MON_IP\" ]]; then\n  log \"ERROR- MON_IP must be defined as the IP address of the monitor\"\n  exit 1\nfi\n\nif [[ -z \"$MON_IP\" || -z \"$CEPH_PUBLIC_NETWORK\" ]]; then\n  log \"ERROR- it looks like we have not been able to discover the network settings\"\n  exit 1\nfi\n\nfunction get_mon_config {\n  # Get fsid from ceph.conf\n  local fsid=$(ceph-conf --lookup fsid -c /etc/ceph/${CLUSTER}.conf)\n\n  timeout=10\n  MONMAP_ADD=\"\"\n\n  while [[ -z \"${MONMAP_ADD// }\" && \"${timeout}\" -gt 0 ]]; do\n    # Get the ceph mon pods (name and IP) from the Kubernetes API. Formatted as a set of monmap params\n    if [[ ${K8S_HOST_NETWORK} -eq 0 ]]; then\n        MONMAP_ADD=$(kubectl get pods --namespace=${NAMESPACE} ${KUBECTL_PARAM} -o template --template=\"{{range .items}}{{if .status.podIP}}--add {{.metadata.name}} {{.status.podIP}} {{end}} {{end}}\")\n    else\n        MONMAP_ADD=$(kubectl get pods --namespace=${NAMESPACE} ${KUBECTL_PARAM} -o template --template=\"{{range .items}}{{if .status.podIP}}--add {{.spec.nodeName}} {{.status.podIP}} {{end}} {{end}}\")\n    fi\n    (( timeout-- ))\n    sleep 1\n  done\n\n  if [[ -z \"${MONMAP_ADD// }\" ]]; then\n      exit 1\n  fi\n\n  # if monmap exists and the mon is already there, don't overwrite monmap\n  if [ -f \"${MONMAP}\" ]; then\n      monmaptool --print \"${MONMAP}\" |grep -q \"${MON_IP// }\"\":6789\"\n      if [ $? -eq 0 ]; then\n          log \"${MON_IP} already exists in monmap ${MONMAP}\"\n          return\n      fi\n  fi\n\n  # Create a monmap with the Pod Names and IP\n  monmaptool --create ${MONMAP_ADD} --fsid ${fsid} $MONMAP --clobber\n}\n\nget_mon_config $IP_VERSION\n\nchown ceph. /var/log/ceph\n\n# If we don't have a monitor keyring, this is a new monitor\nif [[ (! -e \"${MON_DATA_DIR}/keyring\") && (! -e \"${MON_DATA_DIR}/done\") ]]; then\n  if [ ! -e ${MON_KEYRING}.seed ]; then\n    log \"ERROR- ${MON_KEYRING}.seed must exist.  You can extract it from your current monitor by running 'ceph auth get mon. -o $MON_KEYRING' or use a KV Store\"\n    exit 1\n  else\n    cp -vf ${MON_KEYRING}.seed ${MON_KEYRING}\n  fi\n\n  if [ ! -e $MONMAP ]; then\n    log \"ERROR- $MONMAP must exist.  You can extract it from your current monitor by running 'ceph mon getmap -o $MONMAP' or use a KV Store\"\n    exit 1\n  fi\n\n  # Testing if it's not the first monitor, if one key doesn't exist we assume none of them exist\n  for keyring in $OSD_BOOTSTRAP_KEYRING $MDS_BOOTSTRAP_KEYRING $RGW_BOOTSTRAP_KEYRING $ADMIN_KEYRING; do\n    ceph-authtool $MON_KEYRING --import-keyring $keyring\n  done\n\n  # Prepare the monitor daemon's directory with the map and keyring\n  ceph-mon --setuser ceph --setgroup ceph --cluster ${CLUSTER} --mkfs -i ${MON_NAME} --monmap $MONMAP --keyring $MON_KEYRING --mon-data \"$MON_DATA_DIR\"\n\n  touch ${MON_DATA_DIR}/done\nelse\n  log \"Trying to get the most recent monmap...\"\n  # Ignore when we timeout, in most cases that means the cluster has no quorum or\n  # no mons are up and running yet\n  timeout 5 ceph ${CLI_OPTS} mon getmap -o $MONMAP || true\n  ceph-mon --setuser ceph --setgroup ceph --cluster ${CLUSTER} -i ${MON_NAME} --inject-monmap $MONMAP --keyring $MON_KEYRING --mon-data \"$MON_DATA_DIR\"\n  timeout 7 ceph ${CLI_OPTS} mon add \"${MON_NAME}\" \"${MON_IP}:6789\" || true\nfi\n\nlog \"SUCCESS\"\n\n# start MON\nexec /usr/bin/ceph-mon $DAEMON_OPTS -i ${MON_NAME} --mon-data \"$MON_DATA_DIR\" --public-addr \"${MON_IP}:6789\"\n\n",
    "start_osd.sh": "#!/bin/bash\nset -ex\nexport LC_ALL=C\n\nsource variables_entrypoint.sh\nsource common_functions.sh\n\nif is_available rpm; then\n  OS_VENDOR=redhat\n  source /etc/sysconfig/ceph\nelif is_available dpkg; then\n  OS_VENDOR=ubuntu\n  source /etc/default/ceph\nfi\n\nfunction osd_trying_to_determine_scenario {\n  if [ -z \"${OSD_DEVICE}\" ]; then\n    log \"Bootstrapped OSD(s) found; using OSD directory\"\n    source osd_directory.sh\n    osd_directory\n  elif $(parted --script ${OSD_DEVICE} print | egrep -sq '^ 1.*ceph data'); then\n    # parted will trigger udev, so make sure all current udev event be handled.\n    udevadm settle --timeout=600 && partprobe ${OSD_DEVICE} && \\\n      udevadm settle --timeout=600\n    log \"Bootstrapped OSD found; activating ${OSD_DEVICE}\"\n    source osd_disk_activate.sh\n    osd_activate\n  else\n    log \"Device detected, assuming ceph-disk scenario is desired\"\n    log \"Preparing and activating ${OSD_DEVICE}\"\n    osd_disk\n  fi\n}\n\nfunction start_osd {\n  if [[ ! -e /etc/ceph/${CLUSTER}.conf ]]; then\n    log \"ERROR- /etc/ceph/${CLUSTER}.conf must exist; get it from your existing mon\"\n    exit 1\n  fi\n\n  if [ ${CEPH_GET_ADMIN_KEY} -eq 1 ]; then\n    if [[ ! -e $ADMIN_KEYRING ]]; then\n        log \"ERROR- $ADMIN_KEYRING must exist; get it from your existing mon\"\n        exit 1\n    fi\n  fi\n\n  case \"$OSD_TYPE\" in\n    directory)\n      source osd_directory.sh\n      source osd_common.sh\n      osd_directory\n      ;;\n    directory_single)\n      source osd_directory_single.sh\n      osd_directory_single\n      ;;\n    disk)\n      osd_disk\n      ;;\n    prepare)\n      source osd_disk_prepare.sh\n      osd_disk_prepare\n      ;;\n    activate)\n      source osd_disk_activate.sh\n      osd_activate\n      ;;\n    devices)\n      source osd_disks.sh\n      source osd_common.sh\n      osd_disks\n      ;;\n    activate_journal)\n      source osd_activate_journal.sh\n      source osd_common.sh\n      osd_activate_journal\n      ;;\n    *)\n      osd_trying_to_determine_scenario\n      ;;\n  esac\n}\n\nfunction osd_disk {\n  source osd_disk_prepare.sh\n  source osd_disk_activate.sh\n  osd_disk_prepare\n  osd_activate\n}\n\nfunction valid_scenarios {\n  log \"Valid values for CEPH_DAEMON are $ALL_SCENARIOS.\"\n  log \"Valid values for the daemon parameter are $ALL_SCENARIOS\"\n}\n\nfunction invalid_ceph_daemon {\n  if [ -z \"$CEPH_DAEMON\" ]; then\n    log \"ERROR- One of CEPH_DAEMON or a daemon parameter must be defined as the name of the daemon you want to deploy.\"\n    valid_scenarios\n    exit 1\n  else\n    log \"ERROR- unrecognized scenario.\"\n    valid_scenarios\n  fi\n}\n\n###############\n# CEPH_DAEMON #\n###############\n\n# If we are given a valid first argument, set the\n# CEPH_DAEMON variable from it\ncase \"$CEPH_DAEMON\" in\n  osd)\n    # TAG: osd\n    start_osd\n    ;;\n  osd_directory)\n    # TAG: osd_directory\n    OSD_TYPE=\"directory\"\n    start_osd\n    ;;\n  osd_directory_single)\n    # TAG: osd_directory_single\n    OSD_TYPE=\"directory_single\"\n    start_osd\n    ;;\n  osd_ceph_disk)\n    # TAG: osd_ceph_disk\n    OSD_TYPE=\"disk\"\n    start_osd\n    ;;\n  osd_ceph_disk_prepare)\n    # TAG: osd_ceph_disk_prepare\n    OSD_TYPE=\"prepare\"\n    start_osd\n    ;;\n  osd_ceph_disk_activate)\n    # TAG: osd_ceph_disk_activate\n    OSD_TYPE=\"activate\"\n    start_osd\n    ;;\n  osd_ceph_activate_journal)\n    # TAG: osd_ceph_activate_journal\n    OSD_TYPE=\"activate_journal\"\n    start_osd\n    ;;\n  *)\n    invalid_ceph_daemon\n  ;;\nesac\n\n",
    "start_mds.sh": "#!/bin/bash\nset -ex\nexport LC_ALL=C\n\nsource variables_entrypoint.sh\nsource common_functions.sh\n\nif [[ ! -e /etc/ceph/${CLUSTER}.conf ]]; then\n  log \"ERROR- /etc/ceph/${CLUSTER}.conf must exist; get it from your existing mon\"\n  exit 1\nfi\n\n# Check to see if we are a new MDS\nif [ ! -e $MDS_KEYRING ]; then\n\n  if [ -e $ADMIN_KEYRING ]; then\n     KEYRING_OPT=\"--name client.admin --keyring $ADMIN_KEYRING\"\n  elif [ -e $MDS_BOOTSTRAP_KEYRING ]; then\n     KEYRING_OPT=\"--name client.bootstrap-mds --keyring $MDS_BOOTSTRAP_KEYRING\"\n  else\n    log \"ERROR- Failed to bootstrap MDS: could not find admin or bootstrap-mds keyring.  You can extract it from your current monitor by running 'ceph auth get client.bootstrap-mds -o $MDS_BOOTSTRAP_KEYRING\"\n    exit 1\n  fi\n\n  timeout 10 ceph ${CLI_OPTS} $KEYRING_OPT health || exit 1\n\n  # Generate the MDS key\n  ceph ${CLI_OPTS} $KEYRING_OPT auth get-or-create mds.$MDS_NAME osd 'allow rwx' mds 'allow' mon 'allow profile mds' -o $MDS_KEYRING\n  chown ceph. $MDS_KEYRING\n  chmod 600 $MDS_KEYRING\n\nfi\n\n# NOTE (leseb): having the admin keyring is really a security issue\n# If we need to bootstrap a MDS we should probably create the following on the monitors\n# I understand that this handy to do this here\n# but having the admin key inside every container is a concern\n\n# Create the Ceph filesystem, if necessary\nif [ $CEPHFS_CREATE -eq 1 ]; then\n\n  if [[ ! -e $ADMIN_KEYRING ]]; then\n      log \"ERROR- $ADMIN_KEYRING must exist; get it from your existing mon\"\n      exit 1\n  fi\n\n  if [[ \"$(ceph ${CLI_OPTS} fs ls | grep -c name:.${CEPHFS_NAME},)\" -eq 0 ]]; then\n     # Make sure the specified data pool exists\n     if ! ceph ${CLI_OPTS} osd pool stats ${CEPHFS_DATA_POOL} > /dev/null 2>&1; then\n        ceph ${CLI_OPTS} osd pool create ${CEPHFS_DATA_POOL} ${CEPHFS_DATA_POOL_PG}\n     fi\n\n     # Make sure the specified metadata pool exists\n     if ! ceph ${CLI_OPTS} osd pool stats ${CEPHFS_METADATA_POOL} > /dev/null 2>&1; then\n        ceph ${CLI_OPTS} osd pool create ${CEPHFS_METADATA_POOL} ${CEPHFS_METADATA_POOL_PG}\n     fi\n\n     ceph ${CLI_OPTS} fs new ${CEPHFS_NAME} ${CEPHFS_METADATA_POOL} ${CEPHFS_DATA_POOL}\n  fi\nfi\n\nlog \"SUCCESS\"\n# NOTE: prefixing this with exec causes it to die (commit suicide)\n/usr/bin/ceph-mds $DAEMON_OPTS -i ${MDS_NAME}\n\n",
    "start_rgw.sh": "#!/bin/bash\nset -ex\nexport LC_ALL=C\n\nsource variables_entrypoint.sh\nsource common_functions.sh\n\nif [[ ! -e /etc/ceph/${CLUSTER}.conf ]]; then\n  log \"ERROR- /etc/ceph/${CLUSTER}.conf must exist; get it from your existing mon\"\n  exit 1\nfi\n\nif [ ${CEPH_GET_ADMIN_KEY} -eq 1 ]; then\n  if [[ ! -e $ADMIN_KEYRING ]]; then\n      log \"ERROR- $ADMIN_KEYRING must exist; get it from your existing mon\"\n      exit 1\n  fi\nfi\n\n# Check to see if our RGW has been initialized\nif [ ! -e $RGW_KEYRING ]; then\n\n  if [ ! -e $RGW_BOOTSTRAP_KEYRING ]; then\n    log \"ERROR- $RGW_BOOTSTRAP_KEYRING must exist. You can extract it from your current monitor by running 'ceph auth get client.bootstrap-rgw -o $RGW_BOOTSTRAP_KEYRING'\"\n    exit 1\n  fi\n\n  timeout 10 ceph ${CLI_OPTS} --name client.bootstrap-rgw --keyring $RGW_BOOTSTRAP_KEYRING health || exit 1\n\n  # Generate the RGW key\n  ceph ${CLI_OPTS} --name client.bootstrap-rgw --keyring $RGW_BOOTSTRAP_KEYRING auth get-or-create client.rgw.${RGW_NAME} osd 'allow rwx' mon 'allow rw' -o $RGW_KEYRING\n  chown ceph. $RGW_KEYRING\n  chmod 0600 $RGW_KEYRING\nfi\n\nlog \"SUCCESS\"\n\nRGW_FRONTENDS=\"civetweb port=$RGW_CIVETWEB_PORT\"\nif [ \"$RGW_REMOTE_CGI\" -eq 1 ]; then\n  RGW_FRONTENDS=\"fastcgi socket_port=$RGW_REMOTE_CGI_PORT socket_host=$RGW_REMOTE_CGI_HOST\"\nfi\n\nexec /usr/bin/radosgw $DAEMON_OPTS -n client.rgw.${RGW_NAME} -k $RGW_KEYRING --rgw-socket-path=\"\" --rgw-zonegroup=\"$RGW_ZONEGROUP\" --rgw-zone=\"$RGW_ZONE\" --rgw-frontends=\"$RGW_FRONTENDS\"\n\n",
    "start_mgr.sh": "#!/bin/bash\nset -ex\n\nsource variables_entrypoint.sh\nsource common_functions.sh\n\nif [[ ! -e /usr/bin/ceph-mgr ]]; then\n  log \"ERROR- /usr/bin/ceph-mgr doesn't exist\"\n  sleep infinity\nfi\n\nif [[ ! -e /etc/ceph/${CLUSTER}.conf ]]; then\n  log \"ERROR- /etc/ceph/${CLUSTER}.conf must exist; get it from your existing mon\"\n  exit 1\nfi\n\nif [ ${CEPH_GET_ADMIN_KEY} -eq 1 ]; then\n  if [[ ! -e $ADMIN_KEYRING ]]; then\n      log \"ERROR- $ADMIN_KEYRING must exist; get it from your existing mon\"\n      exit 1\n  fi\nfi\n\n# Check to see if our MGR has been initialized\nif [ ! -e \"$MGR_KEYRING\" ]; then\n    # Create ceph-mgr key\n    timeout 10 ceph ${CLI_OPTS} auth get-or-create mgr.\"$MGR_NAME\" mon 'allow profile mgr' osd 'allow *' mds 'allow *' -o \"$MGR_KEYRING\"\n    chown --verbose ceph. \"$MGR_KEYRING\"\n    chmod 600 \"$MGR_KEYRING\"\nfi\n\nlog \"SUCCESS\"\n\nceph -v\n\n# Env. variables matching the pattern \"<module>_\" will be\n# found and parsed for config-key settings by\n#  ceph config-key set mgr/<module>/<key> <value>\nMODULES_TO_DISABLE=`ceph mgr dump | python -c \"import json, sys; print ' '.join(json.load(sys.stdin)['modules'])\"`\n\nfor module in ${ENABLED_MODULES}; do\n    # This module may have been enabled in the past\n    # remove it from the disable list if present\n    MODULES_TO_DISABLE=${MODULES_TO_DISABLE/$module/}\n\n    options=`env | grep ^${module}_ || true`\n    for option in ${options}; do\n        #strip module name\n        option=${option/${module}_/}\n        key=`echo $option | cut -d= -f1`\n        value=`echo $option | cut -d= -f2`\n        ceph ${CLI_OPTS} config-key set mgr/$module/$key $value\n    done\n    ceph ${CLI_OPTS} mgr module enable ${module} --force\ndone\n\nfor module in $MODULES_TO_DISABLE; do\n  ceph ${CLI_OPTS} mgr module disable ${module}\ndone\n\nlog \"SUCCESS\"\n# start ceph-mgr\nexec /usr/bin/ceph-mgr $DAEMON_OPTS -i \"$MGR_NAME\"\n\n\n",
    "init_rgw_ks.sh": "#!/bin/bash\n\n\n\nset -ex\n\ncp -va /tmp/ceph.conf /etc/ceph/ceph.conf\n\ncat >> /etc/ceph/ceph.conf <<EOF\n\n[client.rgw.${POD_NAME}]\nrgw_frontends = \"civetweb port=${RGW_CIVETWEB_PORT}\"\nrgw_keystone_url = \"${KEYSTONE_URL}\"\nrgw_keystone_admin_user = \"${OS_USERNAME}\"\nrgw_keystone_admin_password = \"${OS_PASSWORD}\"\nrgw_keystone_admin_project = \"${OS_PROJECT_NAME}\"\nrgw_keystone_admin_domain = \"${OS_USER_DOMAIN_NAME}\"\nrgw_keystone_accepted_roles = \"admin, _member_\"\nrgw_keystone_api_version = \"3\"\nrgw_keystone_implicit_tenants = \"true\"\nrgw_s3_auth_use_keystone = \"true\"\nEOF\n\n",
    "watch_mon_health.sh": "#!/bin/bash\nset -ex\nexport LC_ALL=C\n\nsource variables_entrypoint.sh\nsource common_functions.sh\n\nfunction watch_mon_health {\n\n  while [ true ]\n  do\n    log \"checking for zombie mons\"\n    CLUSTER=$CLUSTER /check_zombie_mons.py || true\n    log \"sleep 30 sec\"\n    sleep 30\n  done\n}\n\nwatch_mon_health\n\n",
    "variables_entrypoint.sh": "##########################################\n# LIST OF ALL DAEMON SCENARIOS AVAILABLE #\n##########################################\n\nALL_SCENARIOS=\"osd osd_directory osd_directory_single osd_ceph_disk osd_ceph_disk_prepare osd_ceph_disk_activate osd_ceph_activate_journal mgr\"\n\n\n#########################\n# LIST OF ALL VARIABLES #\n#########################\n\n: ${CLUSTER:=ceph}\n: ${CLUSTER_PATH:=ceph-config/${CLUSTER}} # For KV config\n: ${CEPH_CLUSTER_NETWORK:=${CEPH_PUBLIC_NETWORK}}\n: ${CEPH_DAEMON:=${1}} # default daemon to first argument\n: ${CEPH_GET_ADMIN_KEY:=0}\n: ${HOSTNAME:=$(uname -n)}\n: ${MON_NAME:=${HOSTNAME}}\n# (openstack-helm): we need the MONMAP to be stateful, so we retain it\n: ${MONMAP=/etc/ceph/monmap-${CLUSTER}}\n: ${MON_DATA_DIR:=/var/lib/ceph/mon/${CLUSTER}-${MON_NAME}}\n: ${K8S_HOST_NETWORK:=0}\n: ${NETWORK_AUTO_DETECT:=0}\n: ${MDS_NAME:=mds-${HOSTNAME}}\n: ${OSD_FORCE_ZAP:=0}\n: ${OSD_JOURNAL_SIZE:=100}\n: ${OSD_BLUESTORE:=0}\n: ${OSD_DMCRYPT:=0}\n: ${OSD_JOURNAL_UUID:=$(uuidgen)}\n: ${OSD_LOCKBOX_UUID:=$(uuidgen)}\n: ${CRUSH_LOCATION:=root=default host=${HOSTNAME}}\n: ${CEPHFS_CREATE:=0}\n: ${CEPHFS_NAME:=cephfs}\n: ${CEPHFS_DATA_POOL:=${CEPHFS_NAME}_data}\n: ${CEPHFS_DATA_POOL_PG:=8}\n: ${CEPHFS_METADATA_POOL:=${CEPHFS_NAME}_metadata}\n: ${CEPHFS_METADATA_POOL_PG:=8}\n: ${RGW_NAME:=${HOSTNAME}}\n: ${RGW_ZONEGROUP:=}\n: ${RGW_ZONE:=}\n: ${RGW_CIVETWEB_PORT:=8080}\n: ${RGW_REMOTE_CGI:=0}\n: ${RGW_REMOTE_CGI_PORT:=9000}\n: ${RGW_REMOTE_CGI_HOST:=0.0.0.0}\n: ${RGW_USER:=\"cephnfs\"}\n: ${MGR_NAME:=${HOSTNAME}}\n: ${MGR_IP:=0.0.0.0}\n\n# This is ONLY used for the CLI calls, e.g: ceph $CLI_OPTS health\nCLI_OPTS=\"--cluster ${CLUSTER}\"\n\n# This is ONLY used for the daemon's startup, e.g: ceph-osd $DAEMON_OPTS\nDAEMON_OPTS=\"--cluster ${CLUSTER} --setuser ceph --setgroup ceph -d\"\n\nMOUNT_OPTS=\"-t xfs -o noatime,inode64\"\n\n# Internal variables\nMDS_KEYRING=/var/lib/ceph/mds/${CLUSTER}-${MDS_NAME}/keyring\nADMIN_KEYRING=/etc/ceph/${CLUSTER}.client.admin.keyring\nMON_KEYRING=/etc/ceph/${CLUSTER}.mon.keyring\nRGW_KEYRING=/var/lib/ceph/radosgw/${RGW_NAME}/keyring\nMGR_KEYRING=/var/lib/ceph/mgr/${CLUSTER}-${MGR_NAME}/keyring\nMDS_BOOTSTRAP_KEYRING=/var/lib/ceph/bootstrap-mds/${CLUSTER}.keyring\nRGW_BOOTSTRAP_KEYRING=/var/lib/ceph/bootstrap-rgw/${CLUSTER}.keyring\nOSD_BOOTSTRAP_KEYRING=/var/lib/ceph/bootstrap-osd/${CLUSTER}.keyring\nOSD_PATH_BASE=/var/lib/ceph/osd/${CLUSTER}\n",
    "check_zombie_mons.py": "#!/usr/bin/python2\nimport re\nimport os\nimport subprocess\nimport json\n\nMON_REGEX = r\"^\\d: ([0-9\\.]*):\\d+/\\d* mon.([^ ]*)$\"\n# kubctl_command = 'kubectl get pods --namespace=${NAMESPACE} -l application=ceph,component=mon -o template --template=\"{ }}range .items{{ \\\\\"{{.metadata.name}}\\\\\": \\\\\"{{.status.podIP}}\\\\\" ,   {{end}} }\"'\nif int(os.getenv('K8S_HOST_NETWORK', 0)) > 0:\n    kubectl_command = 'kubectl get pods --namespace=${NAMESPACE} -l application=ceph,component=mon -o template --template=\"{ {{range  \\$i, \\$v  := .items}} {{ if \\$i}} , {{ end }} \\\\\"{{\\$v.spec.nodeName}}\\\\\": \\\\\"{{\\$v.status.podIP}}\\\\\" {{end}} }\"'\nelse:\n    kubectl_command = 'kubectl get pods --namespace=${NAMESPACE} -l application=ceph,component=mon -o template --template=\"{ {{range  \\$i, \\$v  := .items}} {{ if \\$i}} , {{ end }} \\\\\"{{\\$v.metadata.name}}\\\\\": \\\\\"{{\\$v.status.podIP}}\\\\\" {{end}} }\"'\n\nmonmap_command = \"ceph --cluster=${CLUSTER} mon getmap > /tmp/monmap && monmaptool -f /tmp/monmap --print\"\n\n\ndef extract_mons_from_monmap():\n    monmap = subprocess.check_output(monmap_command, shell=True)\n    mons = {}\n    for line in monmap.split(\"\\n\"):\n        m = re.match(MON_REGEX, line)\n        if m is not None:\n            mons[m.group(2)] = m.group(1)\n    return mons\n\ndef extract_mons_from_kubeapi():\n    kubemap = subprocess.check_output(kubectl_command, shell=True)\n    return json.loads(kubemap)\n\ncurrent_mons = extract_mons_from_monmap()\nexpected_mons = extract_mons_from_kubeapi()\n\nprint \"current mons:\", current_mons\nprint \"expected mons:\", expected_mons\n\nfor mon in current_mons:\n    removed_mon = False\n    if not mon in expected_mons:\n        print \"removing zombie mon \", mon\n        subprocess.call([\"ceph\", \"--cluster\", os.environ[\"CLUSTER\"], \"mon\", \"remove\", mon])\n        removed_mon = True\n    elif current_mons[mon] != expected_mons[mon]: # check if for some reason the ip of the mon changed\n        print \"ip change dedected for pod \", mon\n        subprocess.call([\"kubectl\", \"--namespace\", os.environ[\"CLUSTER\"], \"delete\", \"pod\", mon])\n        removed_mon = True\n        print \"deleted mon %s via the kubernetes api\" % mon\n\n\nif not removed_mon:\n    print \"no zombie mons found ...\"\n",
    "rbd-provisioner.sh": "#!/bin/bash\n\n\n\nset -ex\n\nexec /usr/local/bin/rbd-provisioner -id ${POD_NAME}\n",
    "log_handler.sh": "#!/bin/sh\n# for busybox sh, which is dash\n\nLOG=$1\nDEFAULT_SIZE=$(expr 100 \\* 1024 \\* 1024)\nSIZE=${2:-${DEFAULT_SIZE}}\n\n/usr/bin/tail -F $1 &\n\nwhile true ; do\n    sleep 30\n    if [ $(/bin/stat -c%s $LOG) -gt $SIZE ] ; then\n        mv $LOG $LOG.1\n    fi\ndone\n",
    "ceph-storage-admin-key-cleaner.sh": "#!/bin/bash\n\nset -ex\n\nfor secret in $CEPH_ADMIN_SECRETS_NAME; do\n  kubectl delete secret \\\n  --namespace ${DEPLOYMENT_NAMESPACE} \\\n  --ignore-not-found=true \\\n  ${secret}\ndone\n\nkubectl delete secret \\\n  --namespace ${DEPLOYMENT_NAMESPACE} \\\n  --ignore-not-found=true \\\n  ${PVC_CEPH_STORAGECLASS_ADMIN_SECRET_NAME}\n\n",
    "check_mgr.sh": "#!/bin/bash\n\nset -ex\nexport LC_ALL=C\n\nsource variables_entrypoint.sh\n\nIS_MGR_AVAIL=`ceph ${CLI_OPTS} mgr dump | python -c \"import json, sys; print json.load(sys.stdin)['available']\"`\n\nif [ \"${IS_MGR_AVAIL}\" = True ]; then\n  exit 0\nelse\n  exit 1\nfi\n\n",
    "exec_ceph_commands.sh": "#!/bin/bash\n\nset -ex\nexport LC_ALL=C\n\nMON_NAME=$(kubectl get pods --namespace=${NAMESPACE} -l application=ceph,component=mon -o template --template=\"{{ with index .items 0}}{{ .metadata.name}}{{end}}\")\n\n# Define field separator to ';' and\n# store commands in a bash array\nIFS=';' read -ra CEPH_COMMANDS <<< \"${CEPH_COMMANDS_LIST}\"\nfor ceph_command in \"${CEPH_COMMANDS[@]}\"; do\n    kubectl --namespace=${NAMESPACE} exec ${MON_NAME} -c ceph-mon -- $ceph_command\ndone\n"
  }
}
